1. (C) between -1 and 1
2. (C) Recursive feature elimination
3. (C) hyperplane
4. (D) Support Vector Classifier
5. (C) old coefficient of ‘X’ ÷ 2.205
6. (B) increases
7. (B) Random Forests explains more variance in data then decision trees

8. (A) Principal Components are calculated using supervised learning techniques
   (B) Principal Components are calculated using unsupervised learning techniques
   
9. (A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty index, employment rate, population and living index 
   (D) Identifying different segments of disease based on BMI, blood pressure, cholesterol, blood sugar levels  
   
10.(A) max_depth
   (B) max_features
   (D) min_samples_leaf 

11. What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection. 

Answer:- An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal.

    Inter Quartile Range(IQR):- The interquartile range rule is useful in detecting the presence of outliers. IQR = Q3 – Q1. The first quartile Q1, which represents a quarter of the way through the list of all data. The third quartile Q3, which represents three-quarters of the way through the list of all data
    
	Using the Interquartile Rule to Find Outliers:
	   1)Calculate the interquartile range for the data.
	   2)Multiply the interquartile range (IQR) by 1.5 (a constant used to discern    outliers).
	   3)Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.
	   4)Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.
	   
12) What is the primary difference between bagging and boosting algorithms?	

Answer:- Bagging : a)Simplest way of combining predictions that belong to the same type
                   b)Aim to decrease variance, not bias.
		   c)Each model receives equal weight.	
		   d)Each model is built independently.
		   e)Bagging tries to solve over-fitting problem.
		   f)If the classifier is unstable (high variance), then apply bagging.
				   
         Boosting: a)A way of combining predictions that belong to the different types
                   b)Aim to decrease bias, not variance.
                   c)Models are weighted according to their performance.
                   d)New models are influenced by performance of previously built models.
                   e)Boosting tries to solve underfitting problem.
                   f)If the classifier is stable and simple (high bias) the apply boosting

13) What is adjusted R2 in logistic regression. How is it calculated?

Answer:- Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1−L1/L0, where L0 represents the log likelihood for the "constant-only" model and L1 is the log likelihood for the full model with constant and predictors.
 
14) What is the difference between standardisation and normalisation?

Answer:- The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1.

15) What is cross-validation? Describe one advantage and one disadvantage of using  cross-validation.

Answer:- Cross Validation in Machine Learning is a great technique to deal with overfitting problem in various algorithms. Instead of training our model on one training dataset, we train our model on many datasets.

   Advantage :- we split the dataset into multiple folds and train the algorithm on different folds. This prevents our model from overfitting the training dataset.

   Disadvantage :-  Increases Training Time: Cross Validation drastically increases the training time. Earlier you had to train your model only on one training set, but with Cross Validation you have to train your model on multiple training sets. 





 



 


 				   
				
				   
   





  